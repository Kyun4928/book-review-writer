import requests
from bs4 import BeautifulSoup
import re
from utils.cache import cache

@cache.cached
def get_reviews(title: str, author: str, max_reviews: int = 10, timeout: int = 10) -> list:
    """
    ì£¼ì–´ì§„ ì±… ì œëª©ì„ ê¸°ë°˜ìœ¼ë¡œ YES24 ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë¦¬ë·° í…ìŠ¤íŠ¸ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
    ì±… IDë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ë·° í˜ì´ì§€ì— ì§ì ‘ ì ‘ê·¼í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤.
    ê²°ê³¼ëŠ” ìºì‹œë˜ì–´ ë™ì¼í•œ ìš”ì²­ì— ëŒ€í•´ ì¬ì‚¬ìš©ë©ë‹ˆë‹¤.
    """
    if not title:
        print("âŒ ì±… ì œëª©ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return []

    print("ğŸ“¡ YES24ì—ì„œ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤...")

    try:
        # 1. ê²€ìƒ‰ í˜ì´ì§€ ì ‘ê·¼
        search_url = f"https://www.yes24.com/Product/Search?domain=BOOK&query={author}%20{title}"
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        search_res = requests.get(search_url, headers=headers, timeout=timeout)
        search_res.raise_for_status()  # HTTP ì˜¤ë¥˜ í™•ì¸

        # 2. ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±í•˜ì—¬ ì²« ë²ˆì§¸ ì±…ì˜ data-goods-no ì†ì„± ì°¾ê¸°
        soup = BeautifulSoup(search_res.text, "html.parser")
        first_item = soup.select_one("li[data-goods-no]")

        if not first_item:
            print("âŒ ì±…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì›¹ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return []

        # 3. data-goods-no ì†ì„±ì—ì„œ ìƒí’ˆ ID ì¶”ì¶œ
        goods_id = first_item.get("data-goods-no")
        if not goods_id:
            print("âŒ ìƒí’ˆ IDë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        print(f"âœ“ ìƒí’ˆ ID: {goods_id}")

        # 4. ì±… ìƒì„¸ URL ìƒì„±
        book_url = f"https://www.yes24.com/Product/Goods/{goods_id}"
        print(f"âœ“ ì±… URL: {book_url}")

        # 5. ë¦¬ë·° í˜ì´ì§€ ì§ì ‘ ì ‘ê·¼
        review_url = f"https://www.yes24.com/Product/communityModules/GoodsReviewList/{goods_id}"
        print(f"âœ“ ë¦¬ë·° URL: {review_url}")

        review_res = requests.get(review_url, headers=headers, timeout=timeout)
        if review_res.status_code != 200:
            # ì²« ë²ˆì§¸ ì‹œë„ê°€ ì‹¤íŒ¨í•˜ë©´ ëŒ€ì²´ URL í˜•ì‹ ì‹œë„
            review_url = f"https://www.yes24.com/Product/communityModules/GoodsReviewList?GoodsNumber={goods_id}"
            print(f"âœ“ ëŒ€ì²´ ë¦¬ë·° URL ì‹œë„: {review_url}")
            review_res = requests.get(review_url, headers=headers, timeout=timeout)
            review_res.raise_for_status()

        # 5. ë¦¬ë·° í˜ì´ì§€ íŒŒì‹±
        review_soup = BeautifulSoup(review_res.text, "html.parser")

        # 6. ë‹¤ì–‘í•œ ê°€ëŠ¥í•œ ì„ íƒìë¡œ ë¦¬ë·° ì°¾ê¸°
        possible_selectors = [
            "div.reviewInfoBot.cropContentsReview",  # ì›ë˜ ì„ íƒì
            "div.review_cont",                       # ë‚´ìš© ì»¨í…Œì´ë„ˆ
            "ul.reviewList li p.review_cont",        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ì„ íƒì
            "div.reviewInfoBot p",                   # ì •ë³´ ë¸”ë¡ ë‚´ ë‹¨ë½
            "div.reviewInfoWrap div.reviewInfoBot",  # ë˜í¼ ë‚´ë¶€ ì„ íƒì
            "p.reviewContent"                        # ë‚´ìš© ë‹¨ë½
        ]

        reviews = []
        for selector in possible_selectors:
            review_elements = review_soup.select(selector)
            if review_elements:
                print(f"âœ“ ì„ íƒì '{selector}'ë¡œ ë¦¬ë·° ìš”ì†Œ {len(review_elements)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                temp_reviews = [r.get_text(strip=True) for r in review_elements if r.get_text(strip=True)]
                if temp_reviews:
                    reviews = temp_reviews
                    break

        # 7. ì—¬ì „íˆ ë¦¬ë·°ë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš°, ìƒì„¸ í˜ì´ì§€ ì ‘ê·¼í•˜ì—¬ ì‹œë„
        if not reviews:
            print("â„¹ï¸ ë¦¬ë·° í˜ì´ì§€ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìƒì„¸ í˜ì´ì§€ì—ì„œ ì‹œë„í•©ë‹ˆë‹¤.")
            book_res = requests.get(book_url, headers=headers, timeout=timeout)
            book_soup = BeautifulSoup(book_res.text, "html.parser")

            # ìƒì„¸ í˜ì´ì§€ì—ì„œ ë¦¬ë·° ì„¹ì…˜ ì°¾ê¸°
            for selector in possible_selectors:
                review_elements = book_soup.select(selector)
                if review_elements:
                    print(f"âœ“ ìƒì„¸ í˜ì´ì§€ì—ì„œ ì„ íƒì '{selector}'ë¡œ ë¦¬ë·° ì°¾ìŒ")
                    temp_reviews = [r.get_text(strip=True) for r in review_elements if r.get_text(strip=True)]
                    if temp_reviews:
                        reviews = temp_reviews
                        break

        # 8. ê²°ê³¼ ë°˜í™˜
        if reviews:
            print(f"âœ… ë¦¬ë·° {len(reviews)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ.")
            return reviews[:max_reviews]  # ìµœëŒ€ max_reviewsê°œë§Œ ë°˜í™˜
        else:
            print("âŒ ì›¹í˜ì´ì§€ì—ì„œ ë¦¬ë·°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            # ë””ë²„ê¹… íŒŒì¼ ì €ì¥
            with open("review_page_debug.html", "w", encoding="utf-8") as f:
                f.write(review_soup.prettify())
            print("â„¹ï¸ ë””ë²„ê¹…ì„ ìœ„í•´ review_page_debug.html íŒŒì¼ì— í˜ì´ì§€ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            return []

    except requests.exceptions.Timeout:
        print("âŒ ë¦¬ë·° ìˆ˜ì§‘ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return []
    except requests.exceptions.HTTPError as e:
        print(f"âŒ HTTP ì˜¤ë¥˜: {e}")
        return []
    except requests.exceptions.ConnectionError:
        print("âŒ ì„œë²„ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        return []
    except Exception as e:
        print(f"âŒ ë¦¬ë·° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ì˜ ì„¸ë¶€ ì •ë³´ ì¶œë ¥
        import traceback
        print(traceback.format_exc())
        return []